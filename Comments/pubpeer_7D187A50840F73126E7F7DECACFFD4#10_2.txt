#6
The "correction" suggested by Leucanella Acutissima does not seem appropriate, as it only considers the 10 p-values from the table in the Appendix, and not the p-values of all the 114 drugs tested. The Benjamini-Hochberg methods needs all p-values, not just a selection of lowest ones.
Regardless of this, there is probably some signal in the data: with 114 tests performed, one expects around 1.14 of these to have a p-value below 0.01 "by chance" (i.e. if all null hypotheses are true), whereas the authors report 7.
#7
Prof. Huber, that is an interesting point. However, as far as I can tell, it does not seem that the results reported in Figure 1C (and later Figures) come from the original screening experiments that use all 114 drugs. Figure 1C appears to be a follow-up experiment, performed separately, using a smaller set of drugs, and a different cell line. Are you suggesting that all subsequent experiments and statistical analyses should continue to be constrained by the n = 114 of the original experiment? (If so, how would one derive p-values for treatments that were not included in later experiments?) Any clarification you can offer would be helpful.
Dear Leucanella Acutissima, if the p-values result from an independent follow-up experiment with only these 10 drugs, then you are right. However, as far as I currently read the paper, these 10 drugs were found in the primary screen, since they match the drugs mentioned in the bottom right of Page 2, where the primary screen is described. Secondary experiments are described on the top left of Page 3, but these were apparently done on 16 drugs ("5 structurally different MEK inhibitors and three structurally different ERK inhibitors", "five structurally different HSP90 inhibitors and three independent taxol derivatives"). I'd also contend that the paper could be written more clearly on where the data for Fig.1C and the table of p-values come from.
But the much more important question is not on how exactly to apply a Benjamini-Hochberg adjustment (i.e. what is the underlying set of tests), but whether the results are plausible. The main conclusions do not rely on a single set of experiments that resulted in a list of p-values and then that's the end of the analysis. Rather, as far as I understand, there are multiple follow-up experiments that validate findings from the initial screen, and therefore worries about multiple testing on an initial set of p-values are a bit tangential.
#9
Prof. Huber, yes the results certainly could have been described more clearly. It is a bit confusing, though in part it is because there is a lot of data.
My reading is this: the 10 drugs were found in the primary screen of 114 compounds; those results are in Appendix Table S1. Of the 114, there were 31 that passed the first cut-off, defining 14 targets. Upon re-test of those 14 using the same assay, 10-11 re-tested positive. There was no attempt at statistical analysis of those results. Instead, independent secondary tests were performed. Those in Figures 1B and 1C used a different cell line (patient‚Äêderived primary GSCs rather than mouse lung cancer cells), and seem to use only the 10-11 "hits" from the primary screen. The secondary assays you mentioned are yet another distinct round of tests, focusing specifically on additional inhibitors of MEK and ERK.
I fully agree about the "much more important question" -- the multiple follow-up experiments that validate the findings from the initial screen make any concerns about multiple testing corrections largely a moot point.
Many thanks for your contributions to this discussion.
